{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5-1: Handwritten digit recognition (Dense NN, CNN = tf.keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The code in this notebook was generated using [GitHub Copilot](https://github.com/features/copilot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "Generate a PyTorch Lightning MNIST classification model with the following:\n",
    "\n",
    "- Model: 3-layer fully connected network with ReLU activations.\n",
    "- Loss: Cross-entropy.\n",
    "- Accuracy Metrics: Track training, validation, and test accuracy using torchmetrics.\n",
    "- Optimizer: Adam optimizer with learning rate scheduler (StepLR).\n",
    "- Data: MNIST dataset with normalization and transformation to tensor.\n",
    "- Logger: TensorBoardLogger.\n",
    "- Callbacks: LearningRateMonitor, TQDMProgressBar.\n",
    "- Training: Train for 10 epochs with a batch size of 64.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "2024-12-17 00:39:14.313152: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-17 00:39:14.324827: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-17 00:39:14.328458: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-17 00:39:14.337350: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-17 00:39:15.098073: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | Sequential         | 109 K  | train\n",
      "1 | train_acc | MulticlassAccuracy | 0      | train\n",
      "2 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "3 | val_acc   | MulticlassAccuracy | 0      | train\n",
      "---------------------------------------------------------\n",
      "109 K     Trainable params\n",
      "0         Non-trainable params\n",
      "109 K     Total params\n",
      "0.438     Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g113056077/.pyenv/versions/aiot-hw5/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g113056077/.pyenv/versions/aiot-hw5/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: |          | 0/? [00:00<?, ?it/s]\n",
      "Epoch 0: 100%|██████████| 860/860 [00:12<00:00, 70.84it/s, v_num=0, val_loss=0.228, val_acc=0.933, train_loss=0.402, train_acc=0.881]\n",
      "Epoch 1: 100%|██████████| 860/860 [00:12<00:00, 70.75it/s, v_num=0, val_loss=0.176, val_acc=0.949, train_loss=0.197, train_acc=0.941]\n",
      "Epoch 2: 100%|██████████| 860/860 [00:12<00:00, 71.22it/s, v_num=0, val_loss=0.150, val_acc=0.955, train_loss=0.147, train_acc=0.956]\n",
      "Epoch 3: 100%|██████████| 860/860 [00:12<00:00, 70.51it/s, v_num=0, val_loss=0.128, val_acc=0.962, train_loss=0.121, train_acc=0.964]\n",
      "Epoch 4: 100%|██████████| 860/860 [00:12<00:00, 70.68it/s, v_num=0, val_loss=0.113, val_acc=0.966, train_loss=0.105, train_acc=0.969]\n",
      "Epoch 5: 100%|██████████| 860/860 [00:12<00:00, 70.92it/s, v_num=0, val_loss=0.110, val_acc=0.967, train_loss=0.0936, train_acc=0.972]\n",
      "Epoch 6: 100%|██████████| 860/860 [00:12<00:00, 70.85it/s, v_num=0, val_loss=0.105, val_acc=0.968, train_loss=0.0866, train_acc=0.975]\n",
      "Epoch 7: 100%|██████████| 860/860 [00:12<00:00, 71.35it/s, v_num=0, val_loss=0.104, val_acc=0.969, train_loss=0.0811, train_acc=0.977]\n",
      "Epoch 8: 100%|██████████| 860/860 [00:12<00:00, 71.10it/s, v_num=0, val_loss=0.0994, val_acc=0.971, train_loss=0.0776, train_acc=0.977]\n",
      "Epoch 9: 100%|██████████| 860/860 [00:12<00:00, 71.32it/s, v_num=0, val_loss=0.0988, val_acc=0.972, train_loss=0.0748, train_acc=0.978]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g113056077/.pyenv/versions/aiot-hw5/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:01<00:00, 96.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.97079998254776      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.09634590148925781    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.97079998254776     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.09634590148925781   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.09634590148925781, 'test_acc': 0.97079998254776}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, TQDMProgressBar\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "class MNISTModel(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Define the model architecture\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10),\n",
    "        )\n",
    "\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = self.train_acc(logits.softmax(dim=-1), y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = self.val_acc(logits.softmax(dim=-1), y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = self.test_acc(logits.softmax(dim=-1), y)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"test_acc\", acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "# Data preparation\n",
    "def prepare_data():\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    "    )\n",
    "\n",
    "    dataset = datasets.MNIST(\n",
    "        root=\"data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    mnist_test = datasets.MNIST(\n",
    "        root=\"data\", train=False, download=True, transform=transform\n",
    "    )\n",
    "    mnist_train, mnist_val = random_split(dataset, [55000, 5000])\n",
    "    return mnist_train, mnist_val, mnist_test\n",
    "\n",
    "\n",
    "mnist_train, mnist_val, mnist_test = prepare_data()\n",
    "train_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(mnist_val, batch_size=64)\n",
    "test_loader = DataLoader(mnist_test, batch_size=64)\n",
    "\n",
    "# Logger and Callbacks\n",
    "logger = TensorBoardLogger(\n",
    "    \"logs5-2\",\n",
    "    name=\"mnist_dnn\",\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "progress_bar = TQDMProgressBar(leave=True)\n",
    "\n",
    "# Train the model\n",
    "model = MNISTModel(learning_rate=1e-3)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=logger,\n",
    "    callbacks=[lr_monitor, progress_bar],\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "Replace the dnn model with a CNN model.\n",
    "\n",
    "## Modify\n",
    "\n",
    "- fix incorrect input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | conv1     | Conv2d             | 320    | train\n",
      "1 | conv2     | Conv2d             | 18.5 K | train\n",
      "2 | conv3     | Conv2d             | 73.9 K | train\n",
      "3 | pool      | MaxPool2d          | 0      | train\n",
      "4 | fc1       | Linear             | 147 K  | train\n",
      "5 | fc2       | Linear             | 1.3 K  | train\n",
      "6 | train_acc | MulticlassAccuracy | 0      | train\n",
      "7 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "8 | val_acc   | MulticlassAccuracy | 0      | train\n",
      "---------------------------------------------------------\n",
      "241 K     Trainable params\n",
      "0         Non-trainable params\n",
      "241 K     Total params\n",
      "0.966     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: |          | 0/? [00:00<?, ?it/s]                                \n",
      "Epoch 0: 100%|██████████| 860/860 [00:12<00:00, 68.48it/s, v_num=0, val_loss=0.0561, val_acc=0.982, train_loss=0.173, train_acc=0.945]\n",
      "Epoch 1: 100%|██████████| 860/860 [00:12<00:00, 69.60it/s, v_num=0, val_loss=0.0341, val_acc=0.991, train_loss=0.0391, train_acc=0.988]\n",
      "Epoch 2: 100%|██████████| 860/860 [00:12<00:00, 70.15it/s, v_num=0, val_loss=0.0311, val_acc=0.991, train_loss=0.0238, train_acc=0.992]\n",
      "Epoch 3: 100%|██████████| 860/860 [00:12<00:00, 70.12it/s, v_num=0, val_loss=0.033, val_acc=0.990, train_loss=0.0151, train_acc=0.995] \n",
      "Epoch 4: 100%|██████████| 860/860 [00:12<00:00, 70.31it/s, v_num=0, val_loss=0.0265, val_acc=0.994, train_loss=0.00997, train_acc=0.997]\n",
      "Epoch 5: 100%|██████████| 860/860 [00:12<00:00, 71.58it/s, v_num=0, val_loss=0.0302, val_acc=0.992, train_loss=0.00638, train_acc=0.998]\n",
      "Epoch 6: 100%|██████████| 860/860 [00:12<00:00, 71.20it/s, v_num=0, val_loss=0.0295, val_acc=0.992, train_loss=0.0047, train_acc=0.999] \n",
      "Epoch 7: 100%|██████████| 860/860 [00:12<00:00, 71.07it/s, v_num=0, val_loss=0.0294, val_acc=0.993, train_loss=0.00333, train_acc=0.999]\n",
      "Epoch 8: 100%|██████████| 860/860 [00:12<00:00, 70.91it/s, v_num=0, val_loss=0.0302, val_acc=0.993, train_loss=0.00262, train_acc=0.999]\n",
      "Epoch 9: 100%|██████████| 860/860 [00:12<00:00, 70.78it/s, v_num=0, val_loss=0.0306, val_acc=0.994, train_loss=0.00198, train_acc=1.000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:01<00:00, 99.85it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9937000274658203     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.023018453270196915    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9937000274658203    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.023018453270196915   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.023018453270196915, 'test_acc': 0.9937000274658203}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, TQDMProgressBar\n",
    "import torchmetrics\n",
    "\n",
    "class CNNModel(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Define the CNN architecture\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "        # Metrics\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten dynamically\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = self.train_acc(logits.softmax(dim=-1), y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = self.val_acc(logits.softmax(dim=-1), y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = self.test_acc(logits.softmax(dim=-1), y)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"test_acc\", acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "# Data preparation\n",
    "def prepare_data():\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    "    )\n",
    "\n",
    "    dataset = datasets.MNIST(\n",
    "        root=\"data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    mnist_test = datasets.MNIST(\n",
    "        root=\"data\", train=False, download=True, transform=transform\n",
    "    )\n",
    "    mnist_train, mnist_val = random_split(dataset, [55000, 5000])\n",
    "    return mnist_train, mnist_val, mnist_test\n",
    "\n",
    "\n",
    "mnist_train, mnist_val, mnist_test = prepare_data()\n",
    "train_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(mnist_val, batch_size=64)\n",
    "test_loader = DataLoader(mnist_test, batch_size=64)\n",
    "\n",
    "# Logger and Callbacks\n",
    "logger = TensorBoardLogger(\n",
    "    \"logs5-2\",\n",
    "    name=\"mnist_cnn\",\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "progress_bar = TQDMProgressBar(leave=True)\n",
    "\n",
    "# Train the model\n",
    "model = CNNModel(learning_rate=1e-3)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=logger,\n",
    "    callbacks=[lr_monitor, progress_bar],\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 159275), started 0:00:22 ago. (Use '!kill 159275' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4e8dc1db4a20d802\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4e8dc1db4a20d802\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir \"logs5-2\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiot-hw5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
