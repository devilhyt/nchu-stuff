{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5-1: Iris Classification Problem (using tf.keras, PyTorch, and PyTorch Lightning, with TensorBoard visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The code in this notebook was generated using [GitHub Copilot](https://github.com/features/copilot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "Generate a TensorFlow program for a classification task using the Iris dataset. The program should include dataset preprocessing, model creation, training, and evaluation. Incorporate advanced features such as learning rate scheduling, TensorBoard logging, and custom callback implementations.\n",
    "\n",
    "- Dataset: Use the Iris dataset from sklearn.datasets.\n",
    "- Preprocessing: \n",
    "    - Perform one-hot encoding on target labels.\n",
    "    - Split data into training and testing sets (80-20 split).\n",
    "- Model Architecture:\n",
    "    - Sequential model with 2 hidden layers (10 neurons each, ReLU activation).\n",
    "    - Output layer with 3 classes and softmax activation.\n",
    "- Compile: Use Adam optimizer with a learning rate of 0.01, categorical crossentropy as the loss, and accuracy as a metric.\n",
    "- Learning Rate Scheduler: Reduce learning rate after 20 epochs using exponential decay.\n",
    "- TensorBoard: Set up TensorBoard for logging training metrics, including histograms.\n",
    "- Custom Callback: Print the loss at the end of each epoch.\n",
    "- Training: Train the model for 100 epochs with validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 23:46:11.216183: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-16 23:46:11.227454: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-16 23:46:11.230959: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-16 23:46:11.239253: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-16 23:46:11.981599: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g113056077/.pyenv/versions/aiot-hw5/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1734363972.918026  138807 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-16 23:46:12.945435: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 624ms/step - accuracy: 0.3438 - loss: 2.1720Epoch 1: Loss = 1.5765020847320557\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.3565 - loss: 1.7645 - val_accuracy: 0.3000 - val_loss: 1.0225 - learning_rate: 0.0100\n",
      "Epoch 2/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.3438 - loss: 0.9923Epoch 2: Loss = 0.9117698669433594\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5017 - loss: 0.9419 - val_accuracy: 0.7000 - val_loss: 0.8124 - learning_rate: 0.0100\n",
      "Epoch 3/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7188 - loss: 0.7910Epoch 3: Loss = 0.8060577511787415\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6956 - loss: 0.7892 - val_accuracy: 0.7000 - val_loss: 0.7238 - learning_rate: 0.0100\n",
      "Epoch 4/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6875 - loss: 0.7254Epoch 4: Loss = 0.716145396232605\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6633 - loss: 0.7230 - val_accuracy: 0.7000 - val_loss: 0.6553 - learning_rate: 0.0100\n",
      "Epoch 5/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6562 - loss: 0.6454Epoch 5: Loss = 0.6479800939559937\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6700 - loss: 0.6453 - val_accuracy: 0.8333 - val_loss: 0.6048 - learning_rate: 0.0100\n",
      "Epoch 6/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.5981Epoch 6: Loss = 0.597754180431366\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8767 - loss: 0.6041 - val_accuracy: 0.8333 - val_loss: 0.5472 - learning_rate: 0.0100\n",
      "Epoch 7/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7188 - loss: 0.5609Epoch 7: Loss = 0.5079867243766785\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8496 - loss: 0.5301 - val_accuracy: 0.9000 - val_loss: 0.4634 - learning_rate: 0.0100\n",
      "Epoch 8/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9062 - loss: 0.4495Epoch 8: Loss = 0.46328920125961304\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8473 - loss: 0.4687 - val_accuracy: 0.8667 - val_loss: 0.4331 - learning_rate: 0.0100\n",
      "Epoch 9/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9062 - loss: 0.4018Epoch 9: Loss = 0.412501722574234\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9179 - loss: 0.4113 - val_accuracy: 0.8667 - val_loss: 0.4019 - learning_rate: 0.0100\n",
      "Epoch 10/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.3575Epoch 10: Loss = 0.3818013072013855\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9579 - loss: 0.3723 - val_accuracy: 0.8667 - val_loss: 0.3770 - learning_rate: 0.0100\n",
      "Epoch 11/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9062 - loss: 0.4149Epoch 11: Loss = 0.35064274072647095\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9383 - loss: 0.3727 - val_accuracy: 0.9667 - val_loss: 0.3391 - learning_rate: 0.0100\n",
      "Epoch 12/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9375 - loss: 0.3692Epoch 12: Loss = 0.32319584488868713\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9563 - loss: 0.3365 - val_accuracy: 0.9333 - val_loss: 0.3156 - learning_rate: 0.0100\n",
      "Epoch 13/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.2521Epoch 13: Loss = 0.2966788113117218\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9677 - loss: 0.2795 - val_accuracy: 1.0000 - val_loss: 0.2930 - learning_rate: 0.0100\n",
      "Epoch 14/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.2706Epoch 14: Loss = 0.2847054600715637\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9581 - loss: 0.2803 - val_accuracy: 0.9333 - val_loss: 0.2754 - learning_rate: 0.0100\n",
      "Epoch 15/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.2631Epoch 15: Loss = 0.25752249360084534\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9729 - loss: 0.2563 - val_accuracy: 0.9667 - val_loss: 0.2534 - learning_rate: 0.0100\n",
      "Epoch 16/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.2360Epoch 16: Loss = 0.23456348478794098\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9712 - loss: 0.2417 - val_accuracy: 1.0000 - val_loss: 0.2388 - learning_rate: 0.0100\n",
      "Epoch 17/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9375 - loss: 0.2781Epoch 17: Loss = 0.2212531417608261\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9552 - loss: 0.2317 - val_accuracy: 0.9667 - val_loss: 0.2192 - learning_rate: 0.0100\n",
      "Epoch 18/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.2296Epoch 18: Loss = 0.19834989309310913\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9712 - loss: 0.2119 - val_accuracy: 0.9667 - val_loss: 0.2083 - learning_rate: 0.0100\n",
      "Epoch 19/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.1674Epoch 19: Loss = 0.19066336750984192\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9804 - loss: 0.1843 - val_accuracy: 1.0000 - val_loss: 0.1984 - learning_rate: 0.0100\n",
      "Epoch 20/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.2078Epoch 20: Loss = 0.1804208606481552\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9733 - loss: 0.1905 - val_accuracy: 1.0000 - val_loss: 0.1888 - learning_rate: 0.0100\n",
      "Epoch 21/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.1245Epoch 21: Loss = 0.17290982604026794\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9900 - loss: 0.1523 - val_accuracy: 0.9667 - val_loss: 0.1736 - learning_rate: 0.0090\n",
      "Epoch 22/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1779Epoch 22: Loss = 0.15463903546333313\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9819 - loss: 0.1630 - val_accuracy: 0.9000 - val_loss: 0.1839 - learning_rate: 0.0082\n",
      "Epoch 23/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.1391Epoch 23: Loss = 0.14912639558315277\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9775 - loss: 0.1444 - val_accuracy: 0.9667 - val_loss: 0.1612 - learning_rate: 0.0074\n",
      "Epoch 24/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.1184Epoch 24: Loss = 0.1375320553779602\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9860 - loss: 0.1367 - val_accuracy: 0.9667 - val_loss: 0.1550 - learning_rate: 0.0067\n",
      "Epoch 25/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9375 - loss: 0.1702Epoch 25: Loss = 0.1327352076768875\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9704 - loss: 0.1444 - val_accuracy: 0.9667 - val_loss: 0.1517 - learning_rate: 0.0061\n",
      "Epoch 26/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1921Epoch 26: Loss = 0.12862123548984528\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9798 - loss: 0.1430 - val_accuracy: 1.0000 - val_loss: 0.1489 - learning_rate: 0.0055\n",
      "Epoch 27/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1061Epoch 27: Loss = 0.1260901689529419\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9819 - loss: 0.1117 - val_accuracy: 0.9667 - val_loss: 0.1453 - learning_rate: 0.0050\n",
      "Epoch 28/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1223Epoch 28: Loss = 0.12398747354745865\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9767 - loss: 0.1264 - val_accuracy: 1.0000 - val_loss: 0.1452 - learning_rate: 0.0045\n",
      "Epoch 29/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.1154Epoch 29: Loss = 0.12145471572875977\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9892 - loss: 0.1156 - val_accuracy: 0.9667 - val_loss: 0.1381 - learning_rate: 0.0041\n",
      "Epoch 30/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0580Epoch 30: Loss = 0.11882449686527252\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9744 - loss: 0.1078 - val_accuracy: 1.0000 - val_loss: 0.1387 - learning_rate: 0.0037\n",
      "Epoch 31/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.1309Epoch 31: Loss = 0.1164003387093544\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9892 - loss: 0.1153 - val_accuracy: 1.0000 - val_loss: 0.1398 - learning_rate: 0.0033\n",
      "Epoch 32/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9375 - loss: 0.1687Epoch 32: Loss = 0.11632285267114639\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9704 - loss: 0.1318 - val_accuracy: 1.0000 - val_loss: 0.1394 - learning_rate: 0.0030\n",
      "Epoch 33/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9375 - loss: 0.1802Epoch 33: Loss = 0.11311623454093933\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9704 - loss: 0.1326 - val_accuracy: 1.0000 - val_loss: 0.1340 - learning_rate: 0.0027\n",
      "Epoch 34/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0496Epoch 34: Loss = 0.11246244609355927\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9829 - loss: 0.1006 - val_accuracy: 0.9667 - val_loss: 0.1312 - learning_rate: 0.0025\n",
      "Epoch 35/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1268Epoch 35: Loss = 0.11187541484832764\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9765 - loss: 0.1099 - val_accuracy: 0.9667 - val_loss: 0.1297 - learning_rate: 0.0022\n",
      "Epoch 36/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0571Epoch 36: Loss = 0.11006902158260345\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9933 - loss: 0.0945 - val_accuracy: 1.0000 - val_loss: 0.1312 - learning_rate: 0.0020\n",
      "Epoch 37/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1459Epoch 37: Loss = 0.1102295070886612\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9798 - loss: 0.1168 - val_accuracy: 1.0000 - val_loss: 0.1357 - learning_rate: 0.0018\n",
      "Epoch 38/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1118Epoch 38: Loss = 0.10897409170866013\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9819 - loss: 0.1060 - val_accuracy: 1.0000 - val_loss: 0.1326 - learning_rate: 0.0017\n",
      "Epoch 39/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0749Epoch 39: Loss = 0.10794179886579514\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9860 - loss: 0.1049 - val_accuracy: 1.0000 - val_loss: 0.1307 - learning_rate: 0.0015\n",
      "Epoch 40/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1258Epoch 40: Loss = 0.10700145363807678\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9798 - loss: 0.1116 - val_accuracy: 1.0000 - val_loss: 0.1289 - learning_rate: 0.0014\n",
      "Epoch 41/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1308Epoch 41: Loss = 0.10694277286529541\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9819 - loss: 0.1161 - val_accuracy: 0.9667 - val_loss: 0.1264 - learning_rate: 0.0012\n",
      "Epoch 42/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0925Epoch 42: Loss = 0.10663428157567978\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9860 - loss: 0.1048 - val_accuracy: 0.9667 - val_loss: 0.1258 - learning_rate: 0.0011\n",
      "Epoch 43/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.1036Epoch 43: Loss = 0.10580803453922272\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9819 - loss: 0.1018 - val_accuracy: 1.0000 - val_loss: 0.1268 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1293Epoch 44: Loss = 0.10533623397350311\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9767 - loss: 0.1127 - val_accuracy: 1.0000 - val_loss: 0.1287 - learning_rate: 9.0718e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0864Epoch 45: Loss = 0.10505814105272293\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9860 - loss: 0.1006 - val_accuracy: 1.0000 - val_loss: 0.1295 - learning_rate: 8.2085e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.1232Epoch 46: Loss = 0.10532154142856598\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9881 - loss: 0.1063 - val_accuracy: 1.0000 - val_loss: 0.1284 - learning_rate: 7.4274e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0814Epoch 47: Loss = 0.1045302227139473\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9881 - loss: 0.0950 - val_accuracy: 1.0000 - val_loss: 0.1284 - learning_rate: 6.7206e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0705Epoch 48: Loss = 0.10443831980228424\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9829 - loss: 0.0992 - val_accuracy: 1.0000 - val_loss: 0.1290 - learning_rate: 6.0810e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0745Epoch 49: Loss = 0.10426460951566696\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9912 - loss: 0.0973 - val_accuracy: 1.0000 - val_loss: 0.1279 - learning_rate: 5.5023e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0801Epoch 50: Loss = 0.10382365435361862\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9829 - loss: 0.1029 - val_accuracy: 1.0000 - val_loss: 0.1270 - learning_rate: 4.9787e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1089Epoch 51: Loss = 0.10354612022638321\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9819 - loss: 0.1050 - val_accuracy: 1.0000 - val_loss: 0.1264 - learning_rate: 4.5049e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1497Epoch 52: Loss = 0.10342378914356232\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9767 - loss: 0.1183 - val_accuracy: 1.0000 - val_loss: 0.1255 - learning_rate: 4.0762e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0747Epoch 53: Loss = 0.10328590124845505\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9912 - loss: 0.0916 - val_accuracy: 1.0000 - val_loss: 0.1250 - learning_rate: 3.6883e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0901Epoch 54: Loss = 0.10310970991849899\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9829 - loss: 0.1063 - val_accuracy: 1.0000 - val_loss: 0.1249 - learning_rate: 3.3373e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0486Epoch 55: Loss = 0.10302723199129105\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9881 - loss: 0.0820 - val_accuracy: 1.0000 - val_loss: 0.1247 - learning_rate: 3.0197e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1237Epoch 56: Loss = 0.10291562229394913\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9819 - loss: 0.1055 - val_accuracy: 1.0000 - val_loss: 0.1249 - learning_rate: 2.7324e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1330Epoch 57: Loss = 0.10288394242525101\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9767 - loss: 0.1129 - val_accuracy: 1.0000 - val_loss: 0.1253 - learning_rate: 2.4724e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1040Epoch 58: Loss = 0.10267363488674164\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9798 - loss: 0.1033 - val_accuracy: 1.0000 - val_loss: 0.1252 - learning_rate: 2.2371e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0799Epoch 59: Loss = 0.10259465873241425\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9860 - loss: 0.0973 - val_accuracy: 1.0000 - val_loss: 0.1251 - learning_rate: 2.0242e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.1538Epoch 60: Loss = 0.10253588110208511\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9767 - loss: 0.1142 - val_accuracy: 1.0000 - val_loss: 0.1251 - learning_rate: 1.8316e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0807Epoch 61: Loss = 0.1024782806634903\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9912 - loss: 0.0935 - val_accuracy: 1.0000 - val_loss: 0.1250 - learning_rate: 1.6573e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1623Epoch 62: Loss = 0.10240449756383896\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9767 - loss: 0.1262 - val_accuracy: 1.0000 - val_loss: 0.1248 - learning_rate: 1.4996e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.1143Epoch 63: Loss = 0.10233251005411148\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9860 - loss: 0.1094 - val_accuracy: 1.0000 - val_loss: 0.1248 - learning_rate: 1.3569e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.1152Epoch 64: Loss = 0.10229816287755966\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9881 - loss: 0.1070 - val_accuracy: 1.0000 - val_loss: 0.1246 - learning_rate: 1.2277e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1028Epoch 65: Loss = 0.10226418823003769\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9798 - loss: 0.1016 - val_accuracy: 1.0000 - val_loss: 0.1247 - learning_rate: 1.1109e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1055Epoch 66: Loss = 0.10219947248697281\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9798 - loss: 0.1059 - val_accuracy: 1.0000 - val_loss: 0.1246 - learning_rate: 1.0052e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.1035Epoch 67: Loss = 0.10225241631269455\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9860 - loss: 0.1046 - val_accuracy: 1.0000 - val_loss: 0.1243 - learning_rate: 9.0953e-05\n",
      "Epoch 68/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.0848Epoch 68: Loss = 0.10213121771812439\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9798 - loss: 0.1002 - val_accuracy: 1.0000 - val_loss: 0.1244 - learning_rate: 8.2298e-05\n",
      "Epoch 69/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0842Epoch 69: Loss = 0.10209597647190094\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9860 - loss: 0.1006 - val_accuracy: 1.0000 - val_loss: 0.1243 - learning_rate: 7.4466e-05\n",
      "Epoch 70/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0951Epoch 70: Loss = 0.10207842290401459\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9892 - loss: 0.0976 - val_accuracy: 1.0000 - val_loss: 0.1242 - learning_rate: 6.7380e-05\n",
      "Epoch 71/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0686Epoch 71: Loss = 0.10203921794891357\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9912 - loss: 0.0903 - val_accuracy: 1.0000 - val_loss: 0.1241 - learning_rate: 6.0968e-05\n",
      "Epoch 72/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0628Epoch 72: Loss = 0.10201321542263031\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9881 - loss: 0.0922 - val_accuracy: 1.0000 - val_loss: 0.1241 - learning_rate: 5.5166e-05\n",
      "Epoch 73/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0924Epoch 73: Loss = 0.10198494791984558\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9860 - loss: 0.1020 - val_accuracy: 1.0000 - val_loss: 0.1241 - learning_rate: 4.9916e-05\n",
      "Epoch 74/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.0995Epoch 74: Loss = 0.10199034959077835\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9767 - loss: 0.1053 - val_accuracy: 1.0000 - val_loss: 0.1241 - learning_rate: 4.5166e-05\n",
      "Epoch 75/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9688 - loss: 0.1196Epoch 75: Loss = 0.10198178142309189\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9798 - loss: 0.1099 - val_accuracy: 1.0000 - val_loss: 0.1240 - learning_rate: 4.0868e-05\n",
      "Epoch 76/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.0963Epoch 76: Loss = 0.10193361341953278\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9819 - loss: 0.0944 - val_accuracy: 1.0000 - val_loss: 0.1240 - learning_rate: 3.6979e-05\n",
      "Epoch 77/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.0945Epoch 77: Loss = 0.10192053020000458\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9798 - loss: 0.0979 - val_accuracy: 1.0000 - val_loss: 0.1240 - learning_rate: 3.3460e-05\n",
      "Epoch 78/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.1076Epoch 78: Loss = 0.10193195939064026\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9912 - loss: 0.0951 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 3.0276e-05\n",
      "Epoch 79/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0716Epoch 79: Loss = 0.10189954191446304\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9860 - loss: 0.0929 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 2.7394e-05\n",
      "Epoch 80/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.1002Epoch 80: Loss = 0.10189787298440933\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9892 - loss: 0.1053 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 2.4788e-05\n",
      "Epoch 81/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9375 - loss: 0.1654Epoch 81: Loss = 0.10188557952642441\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9704 - loss: 0.1161 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 2.2429e-05\n",
      "Epoch 82/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.1121Epoch 82: Loss = 0.10186801105737686\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9860 - loss: 0.1055 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 2.0294e-05\n",
      "Epoch 83/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.0916Epoch 83: Loss = 0.10186048597097397\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9767 - loss: 0.1069 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 1.8363e-05\n",
      "Epoch 84/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0820Epoch 84: Loss = 0.10185321420431137\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9860 - loss: 0.0936 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 1.6616e-05\n",
      "Epoch 85/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.0998Epoch 85: Loss = 0.10184420645236969\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9798 - loss: 0.1012 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 1.5034e-05\n",
      "Epoch 86/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1460Epoch 86: Loss = 0.10183839499950409\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9819 - loss: 0.1104 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 1.3604e-05\n",
      "Epoch 87/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1081Epoch 87: Loss = 0.1018349751830101\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9767 - loss: 0.1059 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 1.2309e-05\n",
      "Epoch 88/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1608Epoch 88: Loss = 0.10182983428239822\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9819 - loss: 0.1178 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 1.1138e-05\n",
      "Epoch 89/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0952Epoch 89: Loss = 0.1018327996134758\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9912 - loss: 0.0974 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 1.0078e-05\n",
      "Epoch 90/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1362Epoch 90: Loss = 0.10182712227106094\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9798 - loss: 0.1120 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 9.1188e-06\n",
      "Epoch 91/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0891Epoch 91: Loss = 0.10182051360607147\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9860 - loss: 0.0978 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 8.2511e-06\n",
      "Epoch 92/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0801Epoch 92: Loss = 0.10181465744972229\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9860 - loss: 0.0960 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 7.4659e-06\n",
      "Epoch 93/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1121Epoch 93: Loss = 0.10181162506341934\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9798 - loss: 0.1062 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 6.7554e-06\n",
      "Epoch 94/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.1084Epoch 94: Loss = 0.10180578380823135\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9819 - loss: 0.1018 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 6.1125e-06\n",
      "Epoch 95/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.1161Epoch 95: Loss = 0.10180384665727615\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9767 - loss: 0.1053 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 5.5308e-06\n",
      "Epoch 96/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0653Epoch 96: Loss = 0.1018022745847702\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9860 - loss: 0.0912 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 5.0045e-06\n",
      "Epoch 97/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.1026Epoch 97: Loss = 0.10179899632930756\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9881 - loss: 0.1097 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 4.5283e-06\n",
      "Epoch 98/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.1075Epoch 98: Loss = 0.1017981618642807\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9860 - loss: 0.1049 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 4.0974e-06\n",
      "Epoch 99/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.0764Epoch 99: Loss = 0.10179540514945984\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9767 - loss: 0.0979 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 3.7074e-06\n",
      "Epoch 100/100\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0793Epoch 100: Loss = 0.10179443657398224\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9881 - loss: 0.0917 - val_accuracy: 1.0000 - val_loss: 0.1239 - learning_rate: 3.3546e-06\n",
      "1/1 - 0s - 20ms/step - accuracy: 1.0000 - loss: 0.1239\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Load and preprocess the Iris dataset\n",
    "iris_data = load_iris()\n",
    "X = iris_data.data  # Features\n",
    "y = iris_data.target.reshape(-1, 1)  # Target reshaped for one-hot encoding\n",
    "\n",
    "# One-hot encoding of target values\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(10, input_shape=(X_train.shape[1],), activation='relu', name='hidden_layer_1'),\n",
    "    Dense(10, activation='relu', name='hidden_layer_2'),\n",
    "    Dense(3, activation='softmax', name='output_layer')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.01),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 20:\n",
    "        return lr\n",
    "    else:\n",
    "        return (lr * tf.math.exp(-0.1)).numpy()\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "# Setup TensorBoard logging directory\n",
    "log_dir = \"logs5-1/iris_classification_tf/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Custom callback to log loss\n",
    "class LossLoggerCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch+1}: Loss = {logs['loss']}\")\n",
    "\n",
    "loss_logger = LossLoggerCallback()\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    callbacks=[tensorboard_callback, lr_scheduler, loss_logger]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "Convert the code to PyTorch.\n",
    "\n",
    "- Log loss and learning rate to TensorBoard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.104034423828125 | Learning Rate = 0.1\n",
      "Epoch 2: Loss = 1.0930980443954468 | Learning Rate = 0.1\n",
      "Epoch 3: Loss = 1.0499930381774902 | Learning Rate = 0.1\n",
      "Epoch 4: Loss = 0.953551173210144 | Learning Rate = 0.1\n",
      "Epoch 5: Loss = 0.8758890628814697 | Learning Rate = 0.1\n",
      "Epoch 6: Loss = 0.8722573518753052 | Learning Rate = 0.1\n",
      "Epoch 7: Loss = 0.8551760315895081 | Learning Rate = 0.1\n",
      "Epoch 8: Loss = 0.8269813060760498 | Learning Rate = 0.1\n",
      "Epoch 9: Loss = 0.8106176257133484 | Learning Rate = 0.1\n",
      "Epoch 10: Loss = 0.820892870426178 | Learning Rate = 0.1\n",
      "Epoch 11: Loss = 0.7938375473022461 | Learning Rate = 0.1\n",
      "Epoch 12: Loss = 0.7787870764732361 | Learning Rate = 0.1\n",
      "Epoch 13: Loss = 0.7670819759368896 | Learning Rate = 0.1\n",
      "Epoch 14: Loss = 0.7550526261329651 | Learning Rate = 0.1\n",
      "Epoch 15: Loss = 0.7484996914863586 | Learning Rate = 0.1\n",
      "Epoch 16: Loss = 0.7183910012245178 | Learning Rate = 0.1\n",
      "Epoch 17: Loss = 0.7145587205886841 | Learning Rate = 0.1\n",
      "Epoch 18: Loss = 0.6817573308944702 | Learning Rate = 0.1\n",
      "Epoch 19: Loss = 0.64688640832901 | Learning Rate = 0.1\n",
      "Epoch 20: Loss = 0.6084614992141724 | Learning Rate = 0.1\n",
      "Epoch 21: Loss = 0.6468567252159119 | Learning Rate = 0.1\n",
      "Epoch 22: Loss = 0.6078420281410217 | Learning Rate = 0.09048374180359596\n",
      "Epoch 23: Loss = 0.6188668608665466 | Learning Rate = 0.08187307530779818\n",
      "Epoch 24: Loss = 0.5938185453414917 | Learning Rate = 0.07408182206817178\n",
      "Epoch 25: Loss = 0.6131841540336609 | Learning Rate = 0.06703200460356391\n",
      "Epoch 26: Loss = 0.5898173451423645 | Learning Rate = 0.060653065971263326\n",
      "Epoch 27: Loss = 0.5977725982666016 | Learning Rate = 0.05488116360940262\n",
      "Epoch 28: Loss = 0.5967056155204773 | Learning Rate = 0.049658530379140926\n",
      "Epoch 29: Loss = 0.58575439453125 | Learning Rate = 0.044932896411722135\n",
      "Epoch 30: Loss = 0.5885849595069885 | Learning Rate = 0.040656965974059885\n",
      "Epoch 31: Loss = 0.5913078188896179 | Learning Rate = 0.03678794411714421\n",
      "Epoch 32: Loss = 0.5857076644897461 | Learning Rate = 0.03328710836980793\n",
      "Epoch 33: Loss = 0.5830494165420532 | Learning Rate = 0.030119421191220186\n",
      "Epoch 34: Loss = 0.5852533578872681 | Learning Rate = 0.027253179303401237\n",
      "Epoch 35: Loss = 0.5860927104949951 | Learning Rate = 0.024659696394160626\n",
      "Epoch 36: Loss = 0.5839911103248596 | Learning Rate = 0.02231301601484296\n",
      "Epoch 37: Loss = 0.5819998979568481 | Learning Rate = 0.02018965179946552\n",
      "Epoch 38: Loss = 0.5820515751838684 | Learning Rate = 0.018268352405273445\n",
      "Epoch 39: Loss = 0.5829761624336243 | Learning Rate = 0.016529888822158636\n",
      "Epoch 40: Loss = 0.5831872224807739 | Learning Rate = 0.014956861922263488\n",
      "Epoch 41: Loss = 0.5824757814407349 | Learning Rate = 0.013533528323661252\n",
      "Epoch 42: Loss = 0.5815755724906921 | Learning Rate = 0.012245642825298175\n",
      "Epoch 43: Loss = 0.5811220407485962 | Learning Rate = 0.011080315836233373\n",
      "Epoch 44: Loss = 0.5811816453933716 | Learning Rate = 0.010025884372280358\n",
      "Epoch 45: Loss = 0.5814366936683655 | Learning Rate = 0.009071795328941236\n",
      "Epoch 46: Loss = 0.5815688967704773 | Learning Rate = 0.008208499862389865\n",
      "Epoch 47: Loss = 0.5814732909202576 | Learning Rate = 0.007427357821433375\n",
      "Epoch 48: Loss = 0.5812279582023621 | Learning Rate = 0.006720551273974964\n",
      "Epoch 49: Loss = 0.5809678435325623 | Learning Rate = 0.006081006262521785\n",
      "Epoch 50: Loss = 0.5807859301567078 | Learning Rate = 0.0055023220056407115\n",
      "Epoch 51: Loss = 0.5807057023048401 | Learning Rate = 0.0049787068367863835\n",
      "Epoch 52: Loss = 0.5807017683982849 | Learning Rate = 0.00450492023935577\n",
      "Epoch 53: Loss = 0.5807308554649353 | Learning Rate = 0.004076220397836612\n",
      "Epoch 54: Loss = 0.580757737159729 | Learning Rate = 0.0036883167401239917\n",
      "Epoch 55: Loss = 0.5807638168334961 | Learning Rate = 0.0033373269960325995\n",
      "Epoch 56: Loss = 0.5807459950447083 | Learning Rate = 0.0030197383422318424\n",
      "Epoch 57: Loss = 0.5807093977928162 | Learning Rate = 0.002732372244729249\n",
      "Epoch 58: Loss = 0.5806638598442078 | Learning Rate = 0.0024723526470339322\n",
      "Epoch 59: Loss = 0.5806175470352173 | Learning Rate = 0.002237077185616553\n",
      "Epoch 60: Loss = 0.5805761218070984 | Learning Rate = 0.002024191144580433\n",
      "Epoch 61: Loss = 0.5805425643920898 | Learning Rate = 0.0018315638888734126\n",
      "Epoch 62: Loss = 0.5805171728134155 | Learning Rate = 0.0016572675401761198\n",
      "Epoch 63: Loss = 0.5804993510246277 | Learning Rate = 0.001499557682047766\n",
      "Epoch 64: Loss = 0.5804872512817383 | Learning Rate = 0.001356855901220089\n",
      "Epoch 65: Loss = 0.5804792642593384 | Learning Rate = 0.0012277339903068402\n",
      "Epoch 66: Loss = 0.5804739594459534 | Learning Rate = 0.001110899653824227\n",
      "Epoch 67: Loss = 0.580470085144043 | Learning Rate = 0.001005183574463355\n",
      "Epoch 68: Loss = 0.5804668068885803 | Learning Rate = 0.0009095277101695787\n",
      "Epoch 69: Loss = 0.580463707447052 | Learning Rate = 0.0008229747049020001\n",
      "Epoch 70: Loss = 0.5804604887962341 | Learning Rate = 0.0007446583070924315\n",
      "Epoch 71: Loss = 0.5804570913314819 | Learning Rate = 0.0006737946999085443\n",
      "Epoch 72: Loss = 0.5804536938667297 | Learning Rate = 0.0006096746565515614\n",
      "Epoch 73: Loss = 0.580450177192688 | Learning Rate = 0.0005516564420760753\n",
      "Epoch 74: Loss = 0.5804466009140015 | Learning Rate = 0.0004991593906910198\n",
      "Epoch 75: Loss = 0.5804431438446045 | Learning Rate = 0.0004516580942612651\n",
      "Epoch 76: Loss = 0.5804398059844971 | Learning Rate = 0.00040867714384640514\n",
      "Epoch 77: Loss = 0.5804365873336792 | Learning Rate = 0.00036978637164829166\n",
      "Epoch 78: Loss = 0.58043372631073 | Learning Rate = 0.000334596545747126\n",
      "Epoch 79: Loss = 0.5804309248924255 | Learning Rate = 0.0003027554745375803\n",
      "Epoch 80: Loss = 0.5804283618927002 | Learning Rate = 0.00027394448187683583\n",
      "Epoch 81: Loss = 0.5804260969161987 | Learning Rate = 0.0002478752176666348\n",
      "Epoch 82: Loss = 0.5804240107536316 | Learning Rate = 0.0002242867719485793\n",
      "Epoch 83: Loss = 0.5804221034049988 | Learning Rate = 0.00020294306362957258\n",
      "Epoch 84: Loss = 0.5804203748703003 | Learning Rate = 0.0001836304777028899\n",
      "Epoch 85: Loss = 0.5804188251495361 | Learning Rate = 0.00016615572731739272\n",
      "Epoch 86: Loss = 0.5804173946380615 | Learning Rate = 0.0001503439192977566\n",
      "Epoch 87: Loss = 0.5804160833358765 | Learning Rate = 0.00013603680375478874\n",
      "Epoch 88: Loss = 0.5804149508476257 | Learning Rate = 0.00012309119026734756\n",
      "Epoch 89: Loss = 0.5804139971733093 | Learning Rate = 0.0001113775147844798\n",
      "Epoch 90: Loss = 0.5804129838943481 | Learning Rate = 0.00010077854290485061\n",
      "Epoch 91: Loss = 0.5804122090339661 | Learning Rate = 9.118819655545119e-05\n",
      "Epoch 92: Loss = 0.5804114937782288 | Learning Rate = 8.251049232659003e-05\n",
      "Epoch 93: Loss = 0.5804108381271362 | Learning Rate = 7.465858083766757e-05\n",
      "Epoch 94: Loss = 0.5804101824760437 | Learning Rate = 6.755387751938408e-05\n",
      "Epoch 95: Loss = 0.5804096460342407 | Learning Rate = 6.112527611295694e-05\n",
      "Epoch 96: Loss = 0.5804091691970825 | Learning Rate = 5.530843701478307e-05\n",
      "Epoch 97: Loss = 0.5804087519645691 | Learning Rate = 5.004514334406081e-05\n",
      "Epoch 98: Loss = 0.5804082751274109 | Learning Rate = 4.528271828867947e-05\n",
      "Epoch 99: Loss = 0.5804079174995422 | Learning Rate = 4.097349789797846e-05\n",
      "Epoch 100: Loss = 0.5804075598716736 | Learning Rate = 3.707435404590864e-05\n",
      "Test Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the Iris dataset\n",
    "iris_data = load_iris()\n",
    "X = iris_data.data  # Features\n",
    "y = iris_data.target.reshape(-1, 1)  # Target reshaped for one-hot encoding\n",
    "\n",
    "# One-hot encoding of target values\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Define the model\n",
    "class IrisModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IrisModel, self).__init__()\n",
    "        self.hidden_layer_1 = nn.Linear(X_train.shape[1], 10)\n",
    "        self.hidden_layer_2 = nn.Linear(10, 10)\n",
    "        self.output_layer = nn.Linear(10, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden_layer_1(x))\n",
    "        x = torch.relu(self.hidden_layer_2(x))\n",
    "        x = torch.softmax(self.output_layer(x), dim=1)\n",
    "        return x\n",
    "\n",
    "model = IrisModel()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 20:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * np.exp(-0.1)\n",
    "\n",
    "# Setup TensorBoard logging\n",
    "date_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = f\"logs5-1/iris_classification_torch/{date_time}\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, torch.argmax(y_train, dim=1))\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log loss and learning rate to TensorBoard\n",
    "    writer.add_scalar('Loss/train', loss.item(), epoch)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    writer.add_scalar('Learning Rate', current_lr, epoch)\n",
    "\n",
    "    # Adjust learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = scheduler(epoch, param_group['lr'])\n",
    "\n",
    "    # Print loss\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {loss.item()} | Learning Rate = {current_lr}\")\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_test)\n",
    "        val_loss = criterion(val_outputs, torch.argmax(y_test, dim=1))\n",
    "        val_correct = (torch.argmax(val_outputs, dim=1) == torch.argmax(y_test, dim=1)).sum().item()\n",
    "        val_accuracy = val_correct / y_test.size(0)\n",
    "        writer.add_scalar('Loss/validation', val_loss.item(), epoch)\n",
    "        writer.add_scalar('Accuracy/validation', val_accuracy, epoch)\n",
    "\n",
    "# Final evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    correct = (torch.argmax(test_outputs, dim=1) == torch.argmax(y_test, dim=1)).sum().item()\n",
    "    accuracy = correct / y_test.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Close TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "Convert the code to PyTorch Lightning.\n",
    "\n",
    "- Change all logs to be recorded at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type   | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | hidden_layer_1 | Linear | 50     | train\n",
      "1 | hidden_layer_2 | Linear | 110    | train\n",
      "2 | output_layer   | Linear | 33     | train\n",
      "--------------------------------------------------\n",
      "193       Trainable params\n",
      "0         Non-trainable params\n",
      "193       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g113056077/.pyenv/versions/aiot-hw5/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g113056077/.pyenv/versions/aiot-hw5/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/g113056077/.pyenv/versions/aiot-hw5/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 4/4 [00:00<00:00, 172.03it/s, v_num=0, val_loss=0.570, val_accuracy=0.967]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 4/4 [00:00<00:00, 148.66it/s, v_num=0, val_loss=0.570, val_accuracy=0.967]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g113056077/.pyenv/versions/aiot-hw5/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at logs5-1/lightning_logs/version_0/checkpoints/epoch=99-step=400.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at logs5-1/lightning_logs/version_0/checkpoints/epoch=99-step=400.ckpt\n",
      "/home/g113056077/.pyenv/versions/aiot-hw5/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 282.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9666666388511658     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5700286626815796     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9666666388511658    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5700286626815796    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.5700286626815796, 'test_accuracy': 0.9666666388511658}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load and preprocess the Iris dataset\n",
    "iris_data = load_iris()\n",
    "X = iris_data.data  # Features\n",
    "y = iris_data.target.reshape(-1, 1)  # Target reshaped for one-hot encoding\n",
    "\n",
    "# One-hot encoding of target values\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# Define the LightningModule\n",
    "class IrisModel(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.1):\n",
    "        super(IrisModel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.hidden_layer_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden_layer_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden_layer_1(x))\n",
    "        x = F.relu(self.hidden_layer_2(x))\n",
    "        x = F.softmax(self.output_layer(x), dim=1)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X_batch, y_batch = batch\n",
    "        outputs = self(X_batch)\n",
    "        loss = F.cross_entropy(outputs, torch.argmax(y_batch, dim=1))\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, on_step=False)\n",
    "\n",
    "        # Log the learning rate for the current step\n",
    "        lr = self.optimizers().param_groups[0][\"lr\"]\n",
    "        self.log(\"learning_rate\", lr, on_epoch=True, on_step=False)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X_batch, y_batch = batch\n",
    "        outputs = self(X_batch)\n",
    "        loss = F.cross_entropy(outputs, torch.argmax(y_batch, dim=1))\n",
    "        val_correct = (\n",
    "            (torch.argmax(outputs, dim=1) == torch.argmax(y_batch, dim=1)).sum().item()\n",
    "        )\n",
    "        val_accuracy = val_correct / y_batch.size(0)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        self.log(\"val_accuracy\", val_accuracy, prog_bar=True, on_epoch=True, on_step=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            optimizer,\n",
    "            lr_lambda=lambda epoch: 1.0 if epoch < 20 else 0.1 ** (epoch - 19),\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X_batch, y_batch = batch\n",
    "        outputs = self(X_batch)\n",
    "        loss = F.cross_entropy(outputs, torch.argmax(y_batch, dim=1))\n",
    "        test_correct = (\n",
    "            (torch.argmax(outputs, dim=1) == torch.argmax(y_batch, dim=1)).sum().item()\n",
    "        )\n",
    "        test_accuracy = test_correct / y_batch.size(0)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        self.log(\"test_accuracy\", test_accuracy, prog_bar=True, on_epoch=True, on_step=False)\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = IrisModel(input_dim=4, hidden_dim=10, output_dim=3, learning_rate=0.1)\n",
    "\n",
    "# Define the trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100, log_every_n_steps=10, logger=pl.loggers.TensorBoardLogger(\"logs5-1/\")\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "trainer.test(dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4b7cfe3cc57d59d5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4b7cfe3cc57d59d5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir \"logs5-1\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiot-hw5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
